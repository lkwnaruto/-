# encoding=utf-8
import numpy as np
import scipy.io
import scipy.linalg
import sklearn.metrics
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split

"""
这段代码定义了一个函数kernel(ker, X1, X2, gamma)，
用于计算两个数据集（X1和X2）之间的核矩阵。这个函数接收4个参数：
        ker: 表示使用哪种核函数，可选的值包括'primal'（原始数据集）、'linear'（线性核函数）和'rbf'（径向基核函数）。
        X1: 表示第一个数据集，必须提供。
        X2: 表示第二个数据集，可以为空。如果为空，则表示仅对第一个数据集进行计算。
        gamma: 表示径向基核函数的参数。
根据不同的核函数类型，该函数会使用不同的方式计算核矩阵。
        如果ker为'primal'或为空，则返回原始数据集。
        如果ker为'linear'，则使用线性核函数计算核矩阵。
        如果ker为'rbf'，则使用径向基核函数计算核矩阵。
"""
def kernel(ker, X1, X2, gamma):
    K = None

    # 如果ker为'primal'或为空，则返回原始数据集。
    if not ker or ker == 'primal':
        K = X1

     #如果ker为'linear'，则使用线性核函数计算核矩阵。
    elif ker == 'linear':
        if X2 is not None:
            K = sklearn.metrics.pairwise.linear_kernel(np.asarray(X1).T, np.asarray(X2).T)
        else:
            # 用源域和目标域数据构造核矩阵
            K = sklearn.metrics.pairwise.linear_kernel(np.asarray(X1).T)

    #如果ker为'rbf'，则使用径向基核函数计算核矩阵。
    elif ker == 'rbf':
        if X2 is not None:
            K = sklearn.metrics.pairwise.rbf_kernel(np.asarray(X1).T, np.asarray(X2).T, gamma)
        else:
            K = sklearn.metrics.pairwise.rbf_kernel(np.asarray(X1).T, None, gamma)
    return K


class TCA:
    def __init__(self, kernel_type='primal', dim=30, lamb=1, gamma=1):
        '''
        Init func
        :param kernel_type: kernel, values: 'primal' | 'linear' | 'rbf' 选择的核函数类型, 原始核则不进行转换
        :param dim: dimension after transfer 迁移后维度
        :param lamb: lambda value in equation 迁移参数
        :param gamma: kernel bandwidth for rbf kernel 高斯核函数
        '''
        # 实例化参数
        self.kernel_type = kernel_type
        self.dim = dim
        self.lamb = lamb
        self.gamma = gamma


    """
    如下fit()函数中包含TCA的主要计算过程：
    求(KLK+μI)−1KHK的前m个特征值, 所以需要分别求矩阵K(核矩阵), L(MMD引入), I(单位阵), H(中心矩阵).
    step1：首先计算 L 和 H 矩阵
    step2：然后选择一些常用的核函数进行映射（比如线性核、高斯核）计算 K（核矩阵）
    step3：接着求(KLK+μI)−1KHK的前 m 个特征值
    step4：得到源域和目标域的降维后的数据，就可以在上面用传统机器学习方法了
    """
    def fit(self, Xs, Xt):
        '''
        Transform Xs and Xt  输入源领域与目标领域的特征
        :param Xs: ns * n_feature, source feature
        :param Xt: nt * n_feature, target feature
        :return: Xs_new and Xt_new after TCA 输出迁移后的特征
        '''
        # 求X的每一列的范数，并将所有的数据归一化
        X = np.hstack((Xs.T, Xt.T))
        # 其中linalg.norm指的是将X沿着axis=0轴计算(每一行, 每一维特征)向量范数
        # X /范数即进行了归一化
        X /= np.linalg.norm(X, axis=0) # axis = 0 按行计算,得到列的性质

        """
        因为X是原数据经过转置后的结果，
        每一列是一个样本, m为特征维度(X.shape的[0]行数),n为样本个数([1]列数)
        """
        m, n = X.shape
        # print(m,n) # 输出得到   特征维度m: 4096 样本个数n: 1008

## ------------------------------------------------------------------------------
# step1：首先计算 L 和 H 矩阵
        #计算M矩阵（其实就是L矩阵）（MMD引入的矩阵）
        ns, nt = len(Xs), len(Xt)  # 得到L计算公式中的ns,nt的值，即样本个数
        # print(ns,nt) # 源域样本个数Ns: 958 （958即为Office-Caltech10数据集中Amazon在线电商图片的数量）；目标域中用于训练的样本个数Nt: 50

        # e为(958+ 50)=1008*1维矩阵
        # 其中np.ones((ns, 1))代表生成ns行1列的全是1的矩阵;
        # vstack将传入参数(元组)的元素数组按垂直方向进行拼接
        e = np.vstack((1 / ns * np.ones((ns, 1)), -1 / nt * np.ones((nt, 1))))
        M = e * e.T
        # 对M归一化
        # np.linalg.norm求范数,其中fro代表归一化时使用Frobenius范数
        M = M / np.linalg.norm(M, 'fro')

        #算中心矩阵H；公式H = I - 1/n.11T
        """
        其中n为样本个数(1008)
        np.eye(n)生成n*n的单位阵
        """
        H = np.eye(n) - 1 / n * np.ones((n, n))

## ------------------------------------------------------------------------------
# step2：选择一些常用的核函数进行映射（比如线性核、高斯核）计算K（核矩阵）
        # 调用定义好的kernel函数计算核矩阵
        # 本例子用源域和目标域数据构造核矩阵
        K = kernel(self.kernel_type, X, None, gamma=self.gamma)

## ------------------------------------------------------------------------------
#step3：接着求(KLK + μI)−1KHK的前m个特征值（计算矩阵的特征值和特征向量）
        n_eye = m if self.kernel_type == 'primal' else n
        # 计算矩阵的特征值和特征向量 矩阵为 a=K*M*K.T+lambda*eye(n_eye),b=K*H*K.T;  w,V = eig(a,b)
        # a和b都是n*n阶方阵(n=1008样本个数)
        a, b = np.linalg.multi_dot([K, M, K.T]) + self.lamb * np.eye(n_eye), np.linalg.multi_dot([K, H, K.T])

        # 进行特征分解；得到w为1*1008的特征值矩阵, V为1008*1008的特征向量方阵
        """
         scipy.linalg.eig() 对矩阵进行特征分解。
         (方阵A分解为：方阵A的特征向量组成的矩阵Q，对角线元素是特征值的对角矩阵)。
         输入参数为需要计算特征值/特征向量的 两个矩阵(M, M)。
         输出w为特征值, v为归一化（单位“长度”）特征向量。
         
         传入两个矩阵a,b的含义:
         eig函数进行广义特征分解时, 假定待分解矩阵（X）是一个线性方程（AX=B）的解（X=A^(-1) * B）
         对X进行分解时，输入该线性方程的两个系数(A、B)
         
         广义特征问题相关知识: https://blog.csdn.net/m0_55196097/article/details/130045643?spm=1001.2014.3001.5501
        """
        w, V = scipy.linalg.eig(a, b) #w特征值，V特征向量

        """
        此处得到的是最小的特征值, 与论文公式不符合, 重新实验
        此处修改TCA代码取最后50个
        """
        # 根据dim的大小选取前dim小的特征值对应的特征向量组成的矩阵A
        # (A是特征变换矩阵，K是kernel矩阵（可以理解为在新空间下的原始数据）。所以让A和K相乘，使得K可以通过A进行TCA变换)
        # 求特征变换矩阵A
        ind = np.argsort(w) #得到排序后特征值索引
        A = V[:, ind[:self.dim]] #得到排序后特征值索引的前dim(=30)列(最小的30个特征值), 从V中取其对应的特征向量组成矩阵
        # print('leading w ind:{}\n'.format(ind[:tca.dim]))
        # print('shape of A:{}'.format(A.shape)) # shape of A:(1008, 30)

        # 求样本X在隐空间映射的点
        # 求样本变换后矩阵Z
        # 让特征变换矩阵A和核矩阵K相乘，使得K通过A进行TCA变换
        """
        A中的每一列(即A^T的每一行)可以看作一个迁移组件(transfer component), 
        类似PCA等降维方法的组件component。
        K的每一列可以看作一个样本的描述。
        """
        Z = np.dot(A.T, K)
        Z /= np.linalg.norm(Z, axis=0) # 归一化

        # 分割源域与目标域数据, 得到两个域映射在隐空间的点
        # 至此, 迁移过程就完成了
        Xs_new, Xt_new = Z[:, :ns].T, Z[:, ns:].T
        return Xs_new, Xt_new


    def fit_predict(self, Xs, Ys, Xt, Yt):
        '''
        变换Xs和Xt，然后使用1NN对目标进行预测
        :param Xs: ns * n_feature, source feature   Xs是大小为ns*n_feature的矩阵，包含源域的特征，其中ns是样本数量，n_feature是特征数量
        :param Ys: ns * 1, source label;   Ys是包含源域的标签的大小为ns*1的向量
        :param Xt: nt * n_feature, target feature;   是大小为nt*n_feature的矩阵，包含目标域的特征，其中nt是样本数量，n_feature是特征数量
        :param Yt: nt * 1, target label;    Yt是包含目标结构域的标签的大小为nt*1的向量
        :return: Accuracy and predicted_labels on the target domain
        '''

        # 调用fit()函数,该方法将源和目标特征矩阵转换为新的特征空间。变换后的特征矩阵分别存储在Xs_new和Xt_new中
        Xs_new, Xt_new = self.fit(Xs, Xt)

        """
        接下来，在变换后的源特征矩阵Xs_new及其对应的标签Ys.ravel（）上
        训练n_neighbors＝1的K近邻分类器。
        然后，使用该训练后的分类器对变换后的目标特征矩阵Xt_new进行预测，
        并将预测的标签存储在y_pred中。
        
        总结：step1：先将Xs,Xt通过fit()函数中的迁移过程，将其转换为Xs_new, Xt_new，
             step2：然后利用变换后的源特征矩阵Xs_new及其对应的标签Ys.ravel（）上训练n_neighbors＝1的K近邻分类器；（利用新特征空间中的源域数据训练模型f(x)）
             step3：使用训练好的分类器模型f(x),对变换后的目标特征矩阵Xt_new进行预测，并将预测的标签存储在y_pred中.
        """
        clf = KNeighborsClassifier(n_neighbors=1)
        clf.fit(Xs_new, Ys.ravel())
        y_pred = clf.predict(Xt_new)

        """
        检验训练的分类器在目标域的预测性能：
        使用sklearn.metrics.accurcy_score计算预测的分类精度，
        该方法将预测标签y_pred与目标域Yt的真实标签进行比较。
        """
        acc = sklearn.metrics.accuracy_score(Yt, y_pred)

        return acc, y_pred # 返回目标域上的精度和预测标签


    # 现用Xt和Xs创建隐空间, 再把Xt2（测试集样本）映射到这个隐空间
    def fit_new(self, Xs, Xt, Xt2):

        """
        其它部分的代码与fit部分相同, 不再赘述, 只有利用变换矩阵做特征映射时不同。
        fit_new用的变换矩阵A与fit相同, A的计算利用全部训练集的核矩阵K(X,X)得到。
        但fit_new特征映射时用的核矩阵不同, 是训练集与测试集间的K(Xt2, X), 在本节简记为K.
        其中Xt2为测试集样本, Xt2中的样本元素记为y, 样本个数为n3.
        """
        # 在该博客文末，作者对fit_new进行的修改，其在其它数据集上精确度有明显提升。文章地址：https://blog.csdn.net/lagoon_lala/article/details/120800667

        '''
        Map Xt2 to the latent space created from Xt and Xs 将Xt2映射到由Xt和Xs创建的隐空间
        :param Xs : ns * n_feature, source feature
        :param Xt : nt * n_feature, target feature
        :param Xt2: n_s, n_feature, target feature to be mapped；Xt2:n_s，n_feature，要映射的目标特征
        :return: Xt2_new, mapped Xt2 with projection created by Xs and Xt； Xt2_new，映射的Xt2具有由X和Xt创建的投影
        '''
        # Computing projection matrix A from Xs an Xt
        #其中hstack将传入参数(元组)的元素数组按水平方向进行拼接(Xs.T的每一列是一个样本)
        X = np.hstack((Xs.T, Xt.T))
        X /= np.linalg.norm(X, axis=0)
        m, n = X.shape
        ns, nt = len(Xs), len(Xt)
        e = np.vstack((1 / ns * np.ones((ns, 1)), -1 / nt * np.ones((nt, 1))))
        M = e * e.T
        M = M / np.linalg.norm(M, 'fro')
        H = np.eye(n) - 1 / n * np.ones((n, n))
        K = kernel(self.kernel_type, X, None, gamma=self.gamma)
        n_eye = m if self.kernel_type == 'primal' else n
        a, b = np.linalg.multi_dot([K, M, K.T]) + self.lamb * np.eye(n_eye), np.linalg.multi_dot([K, H, K.T])
        w, V = scipy.linalg.eig(a, b)
        ind = np.argsort(w)

        # fit_new用的变换矩阵A与fit相同, A的计算利用全部训练集的核矩阵K(X,X)得到
        A = V[:, ind[:self.dim]]

        # 求样本Xt2的核矩阵：
        # fit_new中K还和fit算Xs, Xt时候的不一样
        # Compute kernel with Xt2 as target and X as source
        Xt2 = Xt2.T
        K = kernel(self.kernel_type, X1=Xt2, X2=X, gamma=self.gamma)

        # 求样本Xt2在隐空间映射的点:
        # New target features
        Xt2_new = K @ A # 得到的Xt2_new即新样本Z的转置, 每行为一个样本

        return Xt2_new

    def fit_predict_new(self, Xt, Xs, Ys, Xt2, Yt2):
        '''
        Transfrom Xt and Xs, get Xs_new
        Transform Xt2 with projection matrix created by Xs and Xt, get Xt2_new
        Make predictions on Xt2_new using classifier trained on Xs_new
        :param Xt: ns * n_feature, target feature
        :param Xs: ns * n_feature, source feature
        :param Ys: ns * 1, source label
        :param Xt2: nt * n_feature, new target feature
        :param Yt2: nt * 1, new target label
        :return: Accuracy and predicted_labels on the target domain
        '''
        Xs_new, _ = self.fit(Xs, Xt)
        Xt2_new = self.fit_new(Xs, Xt, Xt2)
        clf = KNeighborsClassifier(n_neighbors=1)
        clf.fit(Xs_new, Ys.ravel())
        y_pred = clf.predict(Xt2_new)
        acc = sklearn.metrics.accuracy_score(Yt2, y_pred)

        return acc, y_pred


if __name__ == '__main__':
    """
    使用scipy.io.loadmat函数从不同的领域加载数据。
    这些领域在“domains”列表中指定，包含了四个字符串，
    分别表示以MATLAB格式存储的数据文件名。
    """
    domains = ['caltech_decaf.mat', 'amazon_decaf.mat', 'webcam_decaf.mat', 'dslr_decaf.mat']

    """
    然后，该脚本循环遍历列表中的每对领域（不包括相同索引的对），
    对于每一对，它使用loadmat函数加载相应的数据。
    分别将两个领域的数据存储在“src_domain”和“tar_domain”变量中。
    """
    for i in [1]:
        for j in [2]:
            if i != j:
                # 划分数据集;在这里更换数据集时，记得修改文件读取路径
                src, tar = 'C:/Users/ZARD/PycharmProjects/pythonProject/迁移学习/数据集/' + domains[i], 'C:/Users/ZARD/PycharmProjects/pythonProject/迁移学习/数据集/' + domains[j]
                src_domain, tar_domain = scipy.io.loadmat(src), scipy.io.loadmat(tar)
                #print('src_domain type:', type(src_domain), '\nsrc_domain:', src_domain) #检验

                # 修改读取label，比如数据DeCAF的label是'labels'；SURF的label是'label'
                Xs, Ys, Xt, Yt = src_domain['feas'], src_domain['labels'], tar_domain['feas'], tar_domain['labels']
                #print('Xs.T:', Xs.T, '\nXt.T:', Xt.T) #检验

                # 拆分目标数据
                Xt1, Xt2, Yt1, Yt2 = train_test_split(Xt, Yt, train_size=50, stratify=Yt, random_state=42)

                # 创建隐藏空间并使用Xs和Xt1进行评估
                tca = TCA(kernel_type='linear', dim=30, lamb=1, gamma=1)
                # hstack将传入参数(元组)的元素数组按水平方向进行拼接(Xs.T的每一列是一个样本)
                #X = np.hstack((Xs.T, Xt1.T))

                #print('X:', X)
                #print('shape of X:', X.shape, '\nshape of Xs.T:', Xs.T.shape, '\nshape of Xt1.T:', Xt1.T.shape)

                # 训练时使用Xs, Ys, Xt1, Yt1
                acc1, ypre1 = tca.fit_predict(Xs, Ys, Xt1, Yt1)

                # Project and evaluate Xt2 existing projection matrix and classifier
                # 投影并评估Xt2现有投影矩阵和分类器
                acc2, ypre2 = tca.fit_predict_new(Xt1, Xs, Ys, Xt2, Yt2)

    print(f'Accuracy of mapped source and target1 data : {acc1:.3f}')  # 映射源域和目标1数据的准确率达到0.800
    print(f'Accuracy of mapped target2 data            : {acc2:.3f}')  # 预测目标2的准确率达到0.700
    #print('src_domain type:',type(src_domain),'\nsrc_domain:',src_domain)

"""
总结：
    step1：先将Xs, Xt通过fit()函数中的迁移过程，将其转换为Xs_new, Xt_new，
    step2：然后利用变换后的源特征矩阵Xs_new及其对应的标签Ys.ravel（）上训练n_neighbors＝1的K近邻分类器；（利用新特征空间中的源域数据训练模型f(x)）
    step3：使用训练好的分类器模型f(x), 对变换后的目标特征矩阵Xt_new进行预测，并将预测的标签存储在y_pred中.
    step4：检验训练的分类器在目标域的预测性能：使用sklearn.metrics.accurcy_score计算预测的分类精度，该方法将预测标签y_pred与目标域Yt的真实标签进行比较（存在acc中）。
        
"""
